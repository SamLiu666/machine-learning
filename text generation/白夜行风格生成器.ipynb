{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample(p, temperature=1.0):  # 定义采样策略\n",
    "    distribution = np.log(p) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "p = [0.05, 0.2, 0.1, 0.5, 0.15]\n",
    "\n",
    "for i, t in zip(range(4), [0.1, 0.4, 0.8, 1.5]):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.bar(np.arange(5), sample(p, t))\n",
    "    plt.title(\"temperature = %s\" %t, size=16)\n",
    "    plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据：白夜行文本\n",
    "file_path = r\"D:\\machine learning\\text generation\\data\\白夜行.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.643 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典单词，字典数量 235834 16412\n"
     ]
    }
   ],
   "source": [
    "# 分词，构建词典\n",
    "whole = open(file_path, encoding='utf-8').read()\n",
    "all_words = list(jieba.cut(whole, cut_all=False))  # jieba分词\n",
    "words = sorted(list(set(all_words))) # 不重复词\n",
    "# word_indices = dict((word, index) for word, index in enumerate(words))\n",
    "word_indices = dict((word, words.index(word)) for word in words)\n",
    "print(\"字典单词，字典数量\", len(all_words),len(word_indices.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的句子总数: 235804\n"
     ]
    }
   ],
   "source": [
    "# 构建序列长度, 30\n",
    "maxlen = 30\n",
    "sentences = []\n",
    "next_word = []\n",
    "\n",
    "for i in range(0, len(all_words) - maxlen):\n",
    "    sentences.append(all_words[i: i + maxlen]) # 句子\n",
    "    next_word.append(all_words[i + maxlen])  # 句子对应的单词\n",
    "print('提取的句子总数:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235804, 30) (235804,)\n"
     ]
    }
   ],
   "source": [
    "# 构建词向量\n",
    "# x: [sentence, seq_length],235804, 30\n",
    "x = np.zeros((len(sentences), maxlen), dtype='float32') # Embedding的输入是2维张量（句子数，序列长度）\n",
    "y = np.zeros((len(sentences)), dtype='float32') # (235804,)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[i, t] = word_indices[word]\n",
    "    y[i] = word_indices[next_word[i]]\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03 GB\n"
     ]
    }
   ],
   "source": [
    "print(np.round((sys.getsizeof(x) / 1024 / 1024 / 1024), 2), \"GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000001B756DCF1F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000001B756DCF1F8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000001B756DF1EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000001B756DF1EE8>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "main_input = layers.Input(shape=(maxlen, ), dtype='float32') \n",
    "model_1 = layers.Embedding(len(words), 128, input_length=maxlen)(main_input)\n",
    "model_1 = layers.Bidirectional(layers.GRU(256, return_sequences=True))(model_1)\n",
    "model_1 = layers.Bidirectional(layers.GRU(128))(model_1)\n",
    "output = layers.Dense(len(words), activation='softmax')(model_1)  \n",
    "model = keras.models.Model(main_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 30, 128)           2100736   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 30, 512)           592896    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               493056    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16412)             4217884   \n",
      "=================================================================\n",
      "Total params: 7,404,572\n",
      "Trainable params: 7,404,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(lr=3e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 235804 samples\n",
      "Epoch 1/64\n",
      "235804/235804 - 51s - loss: 5.5299\n",
      "Epoch 2/64\n",
      "235804/235804 - 47s - loss: 4.8661\n",
      "Epoch 3/64\n",
      "235804/235804 - 47s - loss: 4.6095\n",
      "Epoch 4/64\n",
      "235804/235804 - 47s - loss: 4.4196\n",
      "Epoch 5/64\n",
      "235804/235804 - 48s - loss: 4.2547\n",
      "Epoch 6/64\n",
      "235804/235804 - 48s - loss: 4.1026\n",
      "Epoch 7/64\n",
      "235804/235804 - 48s - loss: 3.9572\n",
      "Epoch 8/64\n",
      "235804/235804 - 49s - loss: 3.8243\n",
      "Epoch 9/64\n",
      "235804/235804 - 48s - loss: 3.6879\n",
      "Epoch 10/64\n",
      "235804/235804 - 48s - loss: 3.5514\n",
      "Epoch 11/64\n",
      "235804/235804 - 48s - loss: 3.4079\n",
      "Epoch 12/64\n",
      "235804/235804 - 49s - loss: 3.2740\n",
      "Epoch 13/64\n",
      "235804/235804 - 48s - loss: 3.1329\n",
      "Epoch 14/64\n",
      "235804/235804 - 49s - loss: 2.9974\n",
      "Epoch 15/64\n",
      "235804/235804 - 48s - loss: 2.8647\n",
      "Epoch 16/64\n",
      "235804/235804 - 48s - loss: 2.7359\n",
      "Epoch 17/64\n",
      "235804/235804 - 49s - loss: 2.6081\n",
      "Epoch 18/64\n",
      "235804/235804 - 49s - loss: 2.4902\n",
      "Epoch 19/64\n",
      "235804/235804 - 49s - loss: 2.3799\n",
      "Epoch 20/64\n",
      "235804/235804 - 48s - loss: 2.2760\n",
      "Epoch 21/64\n",
      "235804/235804 - 48s - loss: 2.1763\n",
      "Epoch 22/64\n",
      "235804/235804 - 48s - loss: 2.0874\n",
      "Epoch 23/64\n",
      "235804/235804 - 49s - loss: 2.0011\n",
      "Epoch 24/64\n",
      "235804/235804 - 49s - loss: 1.9169\n",
      "Epoch 25/64\n",
      "235804/235804 - 50s - loss: 1.8435\n",
      "Epoch 26/64\n",
      "235804/235804 - 50s - loss: 1.7775\n",
      "Epoch 27/64\n",
      "235804/235804 - 50s - loss: 1.7158\n",
      "Epoch 28/64\n",
      "235804/235804 - 48s - loss: 1.6621\n",
      "Epoch 29/64\n",
      "235804/235804 - 48s - loss: 1.6142\n",
      "Epoch 30/64\n",
      "235804/235804 - 48s - loss: 1.5678\n",
      "Epoch 31/64\n",
      "235804/235804 - 48s - loss: 1.5298\n",
      "Epoch 32/64\n",
      "235804/235804 - 48s - loss: 1.4946\n",
      "Epoch 33/64\n",
      "235804/235804 - 48s - loss: 1.4623\n",
      "Epoch 34/64\n",
      "235804/235804 - 49s - loss: 1.4331\n",
      "Epoch 35/64\n",
      "235804/235804 - 48s - loss: 1.4053\n",
      "Epoch 36/64\n",
      "235804/235804 - 49s - loss: 1.3798\n",
      "Epoch 37/64\n",
      "235804/235804 - 48s - loss: 1.3561\n",
      "Epoch 38/64\n",
      "235804/235804 - 48s - loss: 1.3349\n",
      "Epoch 39/64\n",
      "235804/235804 - 48s - loss: 1.3159\n",
      "Epoch 40/64\n",
      "235804/235804 - 48s - loss: 1.3014\n",
      "Epoch 41/64\n",
      "235804/235804 - 48s - loss: 1.2890\n",
      "Epoch 42/64\n",
      "235804/235804 - 48s - loss: 1.2747\n",
      "Epoch 43/64\n",
      "235804/235804 - 48s - loss: 1.2656\n",
      "Epoch 44/64\n",
      "235804/235804 - 48s - loss: 1.2565\n",
      "Epoch 45/64\n",
      "235804/235804 - 48s - loss: 1.2480\n",
      "Epoch 46/64\n",
      "235804/235804 - 48s - loss: 1.2400\n",
      "Epoch 47/64\n",
      "235804/235804 - 48s - loss: 1.2344\n",
      "Epoch 48/64\n",
      "235804/235804 - 48s - loss: 1.2301\n",
      "Epoch 49/64\n",
      "235804/235804 - 48s - loss: 1.2267\n",
      "Epoch 50/64\n",
      "235804/235804 - 48s - loss: 1.2156\n",
      "Epoch 51/64\n",
      "235804/235804 - 48s - loss: 1.2169\n",
      "Epoch 52/64\n",
      "235804/235804 - 49s - loss: 1.2128\n",
      "Epoch 53/64\n",
      "235804/235804 - 49s - loss: 1.2109\n",
      "Epoch 54/64\n",
      "235804/235804 - 49s - loss: 1.2097\n",
      "Epoch 55/64\n",
      "235804/235804 - 48s - loss: 1.2058\n",
      "Epoch 56/64\n",
      "235804/235804 - 49s - loss: 1.2047\n",
      "Epoch 57/64\n",
      "235804/235804 - 48s - loss: 1.2015\n",
      "Epoch 58/64\n",
      "235804/235804 - 48s - loss: 1.1990\n",
      "Epoch 59/64\n",
      "235804/235804 - 50s - loss: 1.1950\n",
      "Epoch 60/64\n",
      "235804/235804 - 49s - loss: 1.1978\n",
      "Epoch 61/64\n",
      "235804/235804 - 50s - loss: 1.1999\n",
      "Epoch 62/64\n",
      "235804/235804 - 50s - loss: 1.1990\n",
      "Epoch 63/64\n",
      "235804/235804 - 51s - loss: 1.1997\n",
      "Epoch 64/64\n",
      "235804/235804 - 48s - loss: 1.1942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b7fff6cc08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=64, batch_size=1024, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本生成函数\n",
    "def write_2(model, temperature, word_num, begin_sentence):\n",
    "    gg = begin_sentence[:30]\n",
    "    print(''.join(gg), end='/// ')\n",
    "    for _ in range(word_num):\n",
    "        sampled = np.zeros((1, maxlen)) \n",
    "        for t, char in enumerate(gg):\n",
    "            sampled[0, t] = word_indices[char]\n",
    "    \n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        if temperature is None:\n",
    "            next_word = words[np.argmax(preds)]\n",
    "        else:\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = words[next_index]\n",
    "            \n",
    "        gg.append(next_word)\n",
    "        gg = gg[1:]\n",
    "        sys.stdout.write(next_word)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_sentence = \"\"\"本文重点介绍语义解析技术中的Text-to-SQL任务，让机器自动将用户输入的自然语言问题转成数据库可操作的SQL查询语句，实现基于数据库的自动问答能力。\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本文重点介绍语义解析技术中的Text-to-SQL任务，让机\n"
     ]
    }
   ],
   "source": [
    "# begin_sentence = whole[50003: 50100]\n",
    "print(begin_sentence[:30])\n",
    "begin_sentence = list(jieba.cut(begin_sentence, cut_all=False))\n",
    "#print(begin_sentence, len(begin_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本文重点介绍语义解析技术中的Text-to-SQL任务，让机器自动将用户输入的自然语言问题转成数据库可操作的/// "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'语义'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3b6bf5c16df7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwrite_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-39c5873a0b0c>\u001b[0m in \u001b[0;36mwrite_2\u001b[1;34m(model, temperature, word_num, begin_sentence)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0msampled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '语义'"
     ]
    }
   ],
   "source": [
    "write_2(model, None, 300, begin_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model purely from the file\n",
    "new_model = keras.models.load_model('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 官方改编"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 346025 characters\n"
     ]
    }
   ],
   "source": [
    "# 读取并为 py2 compat 解码\n",
    "path_to_file = r\"D:\\machine learning\\text generation\\data\\白夜行.txt\"\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.694 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# jieba 分词\n",
    "text = list(jieba.cut(text, cut_all=False))  # jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r\\n',\n",
       " ' ',\n",
       " '&',\n",
       " \"'\",\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '007',\n",
       " '0K',\n",
       " '0VER',\n",
       " '1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '14Mega',\n",
       " '169800',\n",
       " '2',\n",
       " '2.5',\n",
       " '3',\n",
       " '4',\n",
       " '4126',\n",
       " '5',\n",
       " '5N',\n",
       " '5V',\n",
       " '6',\n",
       " '64K',\n",
       " '7',\n",
       " '8',\n",
       " '88',\n",
       " '9',\n",
       " '98',\n",
       " '999',\n",
       " '=',\n",
       " 'A4',\n",
       " 'AB',\n",
       " 'AB型',\n",
       " 'AI',\n",
       " 'Assembier',\n",
       " 'Assembler',\n",
       " 'Basic',\n",
       " 'CD',\n",
       " 'COURSE',\n",
       " 'CPU',\n",
       " 'Crash',\n",
       " 'DNA',\n",
       " 'DRAM',\n",
       " 'Denny',\n",
       " 'Designmake',\n",
       " 'E',\n",
       " 'Electronics',\n",
       " 'Eleven',\n",
       " 'GAME',\n",
       " 'Godiego',\n",
       " 'Hilite',\n",
       " 'IBM',\n",
       " 'IC',\n",
       " 'Invaders',\n",
       " 'Jump',\n",
       " 'KarasawaYukiho',\n",
       " 'Kirihara',\n",
       " 'L',\n",
       " 'LARK',\n",
       " 'MUGEN',\n",
       " 'Marine',\n",
       " 'Memorix',\n",
       " 'NEC',\n",
       " 'NHK',\n",
       " 'NO',\n",
       " 'NTT',\n",
       " 'No',\n",
       " 'O',\n",
       " 'OFF',\n",
       " 'OK',\n",
       " 'O型',\n",
       " 'PC8001',\n",
       " 'PCB',\n",
       " 'PLAY',\n",
       " 'R',\n",
       " 'RK',\n",
       " 'Reiko',\n",
       " 'Ryouji',\n",
       " 'SAY',\n",
       " 'SH',\n",
       " 'Submarine',\n",
       " 'Submatine',\n",
       " 'Submrine',\n",
       " 'T恤',\n",
       " 'UMA',\n",
       " 'V',\n",
       " 'VIP',\n",
       " 'WEST',\n",
       " 'WORLD',\n",
       " 'Y',\n",
       " 'YES',\n",
       " 'YK',\n",
       " '_',\n",
       " 's',\n",
       " 'space',\n",
       " 'submarine',\n",
       " '×',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '∩',\n",
       " '★',\n",
       " '、',\n",
       " '。',\n",
       " '《',\n",
       " '》',\n",
       " 'ガ',\n",
       " 'ギ',\n",
       " 'ラ',\n",
       " 'レ',\n",
       " '一',\n",
       " '一一',\n",
       " '一丁点儿',\n",
       " '一万',\n",
       " '一万二',\n",
       " '一万二千',\n",
       " '一万五千元',\n",
       " '一万元',\n",
       " '一万八千',\n",
       " '一万多元',\n",
       " '一下',\n",
       " '一下四周',\n",
       " '一下头',\n",
       " '一下子',\n",
       " '一下子全部',\n",
       " '一丝',\n",
       " '一丝不挂',\n",
       " '一两分钟',\n",
       " '一两句话',\n",
       " '一两年',\n",
       " '一两百万',\n",
       " '一个',\n",
       " '一个三十岁',\n",
       " '一个三四岁',\n",
       " '一个个',\n",
       " '一个二十',\n",
       " '一个五十岁',\n",
       " '一个劲儿',\n",
       " '一个半',\n",
       " '一个半月',\n",
       " '一个多',\n",
       " '一个多月',\n",
       " '一个打',\n",
       " '一个月',\n",
       " '一个男孩',\n",
       " '一串',\n",
       " '一九七三年',\n",
       " '一九七九年',\n",
       " '一九七四年',\n",
       " '一九八',\n",
       " '一九八一年',\n",
       " '一九八七年',\n",
       " '一九八九年',\n",
       " '一九八五年',\n",
       " '一九八八年',\n",
       " '一九八六年',\n",
       " '一九八四年',\n",
       " '一事',\n",
       " '一二',\n",
       " '一五一十',\n",
       " '一些',\n",
       " '一人',\n",
       " '一仰',\n",
       " '一件',\n",
       " '一份',\n",
       " '一伙',\n",
       " '一会儿',\n",
       " '一位',\n",
       " '一侧',\n",
       " '一共',\n",
       " '一具',\n",
       " '一册',\n",
       " '一再',\n",
       " '一凛',\n",
       " '一出',\n",
       " '一分',\n",
       " '一分一秒',\n",
       " '一分钟',\n",
       " '一切',\n",
       " '一切都是',\n",
       " '一切顺利',\n",
       " '一列',\n",
       " '一则',\n",
       " '一到',\n",
       " '一刹那',\n",
       " '一刻',\n",
       " '一刻起',\n",
       " '一前一后',\n",
       " '一副',\n",
       " '一动',\n",
       " '一动不动',\n",
       " '一勺',\n",
       " '一区',\n",
       " '一千',\n",
       " '一千万',\n",
       " '一千二',\n",
       " '一千五',\n",
       " '一千多元',\n",
       " '一半',\n",
       " '一半一半',\n",
       " '一厅',\n",
       " '一厉',\n",
       " '一去',\n",
       " '一双',\n",
       " '一叠',\n",
       " '一口',\n",
       " '一口口',\n",
       " '一口气',\n",
       " '一句',\n",
       " '一只',\n",
       " '一台',\n",
       " '一号',\n",
       " '一同',\n",
       " '一名',\n",
       " '一向',\n",
       " '一听',\n",
       " '一听友',\n",
       " '一周',\n",
       " '一周年',\n",
       " '一味',\n",
       " '一命',\n",
       " '一命呜呼',\n",
       " '一团',\n",
       " '一圈',\n",
       " '一在',\n",
       " '一场',\n",
       " '一块',\n",
       " '一声',\n",
       " '一声不吭',\n",
       " '一处',\n",
       " '一大',\n",
       " '一大半',\n",
       " '一大口',\n",
       " '一大堆',\n",
       " '一大排',\n",
       " '一大早',\n",
       " '一大步',\n",
       " '一大笔',\n",
       " '一大笔钱',\n",
       " '一天',\n",
       " '一天到晚',\n",
       " '一天天',\n",
       " '一头',\n",
       " '一头雾水',\n",
       " '一套',\n",
       " '一如',\n",
       " '一定',\n",
       " '一定之规',\n",
       " '一家',\n",
       " '一家人',\n",
       " '一对',\n",
       " '一对一',\n",
       " '一小',\n",
       " '一小块',\n",
       " '一小部分',\n",
       " '一尘不染',\n",
       " '一层',\n",
       " '一届',\n",
       " '一岁',\n",
       " '一带',\n",
       " '一席话',\n",
       " '一幅',\n",
       " '一幕',\n",
       " '一幢',\n",
       " '一年',\n",
       " '一年多来',\n",
       " '一年级',\n",
       " '一并',\n",
       " '一应俱全',\n",
       " '一店',\n",
       " '一度',\n",
       " '一座',\n",
       " '一张',\n",
       " '一弯',\n",
       " '一心',\n",
       " '一惊',\n",
       " '一想',\n",
       " '一成',\n",
       " '一成不变',\n",
       " '一成松',\n",
       " '一成边',\n",
       " '一截',\n",
       " '一户',\n",
       " '一房一厅',\n",
       " '一所',\n",
       " '一扇',\n",
       " '一扇门',\n",
       " '一手',\n",
       " '一手包办',\n",
       " '一打',\n",
       " '一把',\n",
       " '一抹',\n",
       " '一拍',\n",
       " '一拳',\n",
       " '一挂',\n",
       " '一指',\n",
       " '一挤',\n",
       " '一摊',\n",
       " '一撇',\n",
       " '一支',\n",
       " '一整天',\n",
       " '一方',\n",
       " '一方面',\n",
       " '一旁',\n",
       " '一无所知',\n",
       " '一无所获',\n",
       " '一无所觉',\n",
       " '一旦',\n",
       " '一早',\n",
       " '一时',\n",
       " '一时之间',\n",
       " '一时冲动',\n",
       " '一时间',\n",
       " '一是',\n",
       " '一月',\n",
       " '一有',\n",
       " '一望',\n",
       " '一本',\n",
       " '一本正经',\n",
       " '一朵',\n",
       " '一束',\n",
       " '一条',\n",
       " '一杯',\n",
       " '一枝独秀',\n",
       " '一查',\n",
       " '一栋',\n",
       " '一栏',\n",
       " '一株',\n",
       " '一样',\n",
       " '一根',\n",
       " '一桩',\n",
       " '一棒',\n",
       " '一楼',\n",
       " '一模一样',\n",
       " '一次',\n",
       " '一次性',\n",
       " '一款',\n",
       " '一步',\n",
       " '一步步',\n",
       " '一死',\n",
       " '一段',\n",
       " '一段时间',\n",
       " '一段距离',\n",
       " '一段路',\n",
       " '一气之下',\n",
       " '一氧化碳',\n",
       " '一沓',\n",
       " '一流',\n",
       " '一清二楚',\n",
       " '一滴',\n",
       " '一点',\n",
       " '一点一滴',\n",
       " '一点半',\n",
       " '一点点',\n",
       " '一片',\n",
       " '一环',\n",
       " '一班',\n",
       " '一瓶',\n",
       " '一生',\n",
       " '一男一女',\n",
       " '一番',\n",
       " '一番话',\n",
       " '一百',\n",
       " '一百七十',\n",
       " '一百万',\n",
       " '一百万元',\n",
       " '一百九十三',\n",
       " '一百五十',\n",
       " '一百元',\n",
       " '一百八十度',\n",
       " '一百米',\n",
       " '一百遍',\n",
       " '一盒',\n",
       " '一目了然',\n",
       " '一直',\n",
       " '一看',\n",
       " '一眨眼',\n",
       " '一眼',\n",
       " '一睑',\n",
       " '一瞥',\n",
       " '一瞬',\n",
       " '一瞬间',\n",
       " '一碗',\n",
       " '一碰',\n",
       " '一磨',\n",
       " '一票',\n",
       " '一种',\n",
       " '一科',\n",
       " '一秒',\n",
       " '一秒钟',\n",
       " '一窍不通',\n",
       " '一站',\n",
       " '一端',\n",
       " '一笑',\n",
       " '一笔',\n",
       " '一笔一画',\n",
       " '一箭双雕',\n",
       " '一类',\n",
       " '一紧',\n",
       " '一线',\n",
       " '一线生机',\n",
       " '一组',\n",
       " '一经',\n",
       " '一罐',\n",
       " '一群',\n",
       " '一肚子',\n",
       " '一股',\n",
       " '一脚',\n",
       " '一脸',\n",
       " '一腿',\n",
       " '一臂之力',\n",
       " '一致',\n",
       " '一般',\n",
       " '一行',\n",
       " '一袭',\n",
       " '一见钟情',\n",
       " '一览无余',\n",
       " '一角',\n",
       " '一言不发',\n",
       " '一词',\n",
       " '一语',\n",
       " '一语不发',\n",
       " '一语中的',\n",
       " '一说',\n",
       " '一贯',\n",
       " '一起',\n",
       " '一趟',\n",
       " '一路',\n",
       " '一跷',\n",
       " '一身',\n",
       " '一辆',\n",
       " '一辆车',\n",
       " '一辆辆',\n",
       " '一辈',\n",
       " '一辈子',\n",
       " '一边',\n",
       " '一进',\n",
       " '一连串',\n",
       " '一通',\n",
       " '一遍',\n",
       " '一道',\n",
       " '一遭',\n",
       " '一部',\n",
       " '一部分',\n",
       " '一重',\n",
       " '一长串',\n",
       " '一闪而过',\n",
       " '一闭',\n",
       " '一问',\n",
       " '一间',\n",
       " '一阵',\n",
       " '一阵子',\n",
       " '一集',\n",
       " '一震',\n",
       " '一靠',\n",
       " '一面',\n",
       " '一页',\n",
       " '一项',\n",
       " '一顿',\n",
       " '一颗',\n",
       " '一鼓作气',\n",
       " '一齐',\n",
       " '七丁',\n",
       " '七丁目',\n",
       " '七上八下',\n",
       " '七位',\n",
       " '七八年',\n",
       " '七十',\n",
       " '七十二',\n",
       " '七十六',\n",
       " '七号',\n",
       " '七周年',\n",
       " '七层',\n",
       " '七层楼',\n",
       " '七年',\n",
       " '七成',\n",
       " '七旬',\n",
       " '七星',\n",
       " '七月',\n",
       " '七月份',\n",
       " '七次',\n",
       " '七点',\n",
       " '万一',\n",
       " '万一出',\n",
       " '万万',\n",
       " '万元',\n",
       " '万分',\n",
       " '万套',\n",
       " '万宝路',\n",
       " '万能',\n",
       " '丈夫',\n",
       " '三',\n",
       " '三丁目',\n",
       " '三七分',\n",
       " '三三两两',\n",
       " '三下',\n",
       " '三个',\n",
       " '三二',\n",
       " '三人',\n",
       " '三件',\n",
       " '三位',\n",
       " '三分',\n",
       " '三分之一',\n",
       " '三分之二',\n",
       " '三分钟',\n",
       " '三十',\n",
       " '三十一日',\n",
       " '三十万',\n",
       " '三十万元',\n",
       " '三十三',\n",
       " '三十二',\n",
       " '三十二岁',\n",
       " '三十五岁',\n",
       " '三十六岁',\n",
       " '三十几岁',\n",
       " '三十几年',\n",
       " '三十出头',\n",
       " '三十分钟',\n",
       " '三十厘米',\n",
       " '三十多年',\n",
       " '三十好几',\n",
       " '三十岁',\n",
       " '三十年',\n",
       " '三十种',\n",
       " '三千',\n",
       " '三千元',\n",
       " '三协',\n",
       " '三双',\n",
       " '三叠',\n",
       " '三号',\n",
       " '三名',\n",
       " '三四倍',\n",
       " '三四十',\n",
       " '三四天',\n",
       " '三垒',\n",
       " '三声',\n",
       " '三处',\n",
       " '三天',\n",
       " '三天两头',\n",
       " '三头六臂',\n",
       " '三室',\n",
       " '三层',\n",
       " '三层楼',\n",
       " '三居',\n",
       " '三岁',\n",
       " '三年',\n",
       " '三年级',\n",
       " '三张',\n",
       " '三心二意',\n",
       " '三成',\n",
       " '三振出局',\n",
       " '三日',\n",
       " '三明治',\n",
       " '三月',\n",
       " '三条',\n",
       " '三楼',\n",
       " '三次',\n",
       " '三段',\n",
       " '三泽',\n",
       " '三流',\n",
       " '三点',\n",
       " '三班',\n",
       " '三瓶',\n",
       " '三田',\n",
       " '三番两次',\n",
       " '三百',\n",
       " '三百个',\n",
       " '三百五十',\n",
       " '三百名',\n",
       " '三研',\n",
       " '三种',\n",
       " '三箱',\n",
       " '三角形',\n",
       " '三言两语',\n",
       " '三赏',\n",
       " '三道',\n",
       " '三重',\n",
       " '上',\n",
       " '上上星期',\n",
       " '上下',\n",
       " '上个星期',\n",
       " '上个月',\n",
       " '上书',\n",
       " '上列',\n",
       " '上前',\n",
       " '上加',\n",
       " '上升',\n",
       " '上午',\n",
       " '上半身',\n",
       " '上半部',\n",
       " '上去',\n",
       " '上司',\n",
       " '上吉田',\n",
       " '上周',\n",
       " '上场',\n",
       " '上天',\n",
       " '上学',\n",
       " '上工',\n",
       " '上市',\n",
       " '上床',\n",
       " '上开',\n",
       " '上当',\n",
       " '上扬',\n",
       " '上推',\n",
       " '上放',\n",
       " '上方',\n",
       " '上旬',\n",
       " '上星期',\n",
       " '上来',\n",
       " '上桌',\n",
       " '上楼',\n",
       " '上次',\n",
       " '上海',\n",
       " '上演',\n",
       " '上火',\n",
       " '上爬',\n",
       " '上班',\n",
       " '上班族',\n",
       " '上班时间',\n",
       " '上移',\n",
       " '上空',\n",
       " '上策',\n",
       " '上系',\n",
       " '上行',\n",
       " '上衣',\n",
       " '上装',\n",
       " '上见',\n",
       " '上诚',\n",
       " '上课',\n",
       " '上课时',\n",
       " '上身',\n",
       " '上车',\n",
       " '上车时',\n",
       " '上过',\n",
       " '上野',\n",
       " '上铺',\n",
       " '上锁',\n",
       " '上门',\n",
       " '上门来',\n",
       " '上面',\n",
       " '下',\n",
       " '下一惊',\n",
       " '下不为例',\n",
       " '下不来',\n",
       " '下世纪',\n",
       " '下个',\n",
       " '下个星期',\n",
       " '下个月',\n",
       " '下人会',\n",
       " '下北泽',\n",
       " '下午',\n",
       " '下午茶',\n",
       " '下半身',\n",
       " '下单',\n",
       " '下去',\n",
       " '下唇',\n",
       " '下场',\n",
       " '下垂',\n",
       " '下多言',\n",
       " '下学',\n",
       " '下学期',\n",
       " '下定',\n",
       " '下定决心',\n",
       " '下巴',\n",
       " '下床',\n",
       " '下意识',\n",
       " '下手',\n",
       " '下摆',\n",
       " '下方',\n",
       " '下星期',\n",
       " '下月初',\n",
       " '下来',\n",
       " '下楼',\n",
       " '下楼去',\n",
       " '下楼梯',\n",
       " '下榻',\n",
       " '下次',\n",
       " '下毒',\n",
       " '下沉',\n",
       " '下流',\n",
       " '下班',\n",
       " '下着雨',\n",
       " '下箸',\n",
       " '下聘',\n",
       " '下能',\n",
       " '下腹部',\n",
       " '下落',\n",
       " '下课',\n",
       " '下课时间',\n",
       " '下身',\n",
       " '下车',\n",
       " '下车时',\n",
       " '下辈子',\n",
       " '下酒菜',\n",
       " '下降',\n",
       " '下雨',\n",
       " '下雪',\n",
       " '下面',\n",
       " '下颚',\n",
       " '不',\n",
       " '不一会儿',\n",
       " '不一而足',\n",
       " '不三不四',\n",
       " '不上不下',\n",
       " '不下',\n",
       " '不中用',\n",
       " '不为所动',\n",
       " '不为过',\n",
       " '不久',\n",
       " '不久前',\n",
       " '不乏',\n",
       " '不买',\n",
       " '不乱',\n",
       " '不了',\n",
       " '不了了之',\n",
       " '不予',\n",
       " '不争',\n",
       " '不仅',\n",
       " '不仅仅',\n",
       " '不仅如此',\n",
       " '不介意',\n",
       " '不休',\n",
       " '不会',\n",
       " '不但',\n",
       " '不住',\n",
       " '不佳',\n",
       " '不便',\n",
       " '不信',\n",
       " '不倒翁',\n",
       " '不假',\n",
       " '不假思索',\n",
       " '不做声',\n",
       " '不停',\n",
       " '不像',\n",
       " '不像会',\n",
       " '不光',\n",
       " '不免',\n",
       " '不免有些',\n",
       " '不全',\n",
       " '不全是',\n",
       " '不具',\n",
       " '不再',\n",
       " '不准',\n",
       " '不出',\n",
       " '不出所料',\n",
       " '不利',\n",
       " '不到',\n",
       " '不动',\n",
       " '不动产',\n",
       " '不动声色',\n",
       " '不单',\n",
       " '不单单是',\n",
       " '不去',\n",
       " '不及',\n",
       " '不发',\n",
       " '不受',\n",
       " '不变',\n",
       " '不可',\n",
       " '不可一世',\n",
       " '不可同日而语',\n",
       " '不可开交',\n",
       " '不可思议',\n",
       " '不可或缺',\n",
       " '不合',\n",
       " '不合理',\n",
       " '不合身',\n",
       " '不同',\n",
       " '不同于',\n",
       " '不同凡响',\n",
       " '不向',\n",
       " '不向筱冢',\n",
       " '不含税',\n",
       " '不听使唤',\n",
       " '不听话',\n",
       " '不吭声',\n",
       " '不咸不淡',\n",
       " '不在少数',\n",
       " '不在意',\n",
       " '不堪',\n",
       " '不堪设想',\n",
       " '不多',\n",
       " '不够',\n",
       " '不够格',\n",
       " '不大远',\n",
       " '不太',\n",
       " '不太会',\n",
       " '不太像',\n",
       " '不太可能',\n",
       " '不太好',\n",
       " '不太对',\n",
       " '不太强',\n",
       " '不太想',\n",
       " '不太懂',\n",
       " '不太有',\n",
       " '不太脏',\n",
       " '不失',\n",
       " '不好',\n",
       " '不好受',\n",
       " '不好意思',\n",
       " '不如',\n",
       " '不如说',\n",
       " '不如说是',\n",
       " '不妙',\n",
       " '不妥',\n",
       " '不妨',\n",
       " '不孕',\n",
       " '不安',\n",
       " '不安分',\n",
       " '不宜居住',\n",
       " '不容',\n",
       " '不对劲',\n",
       " '不小',\n",
       " '不少',\n",
       " '不屑',\n",
       " '不巧',\n",
       " '不差',\n",
       " '不已',\n",
       " '不带',\n",
       " '不干',\n",
       " '不干不净',\n",
       " '不幸',\n",
       " '不幸遇害',\n",
       " '不应',\n",
       " '不开',\n",
       " '不归',\n",
       " '不得',\n",
       " '不得不',\n",
       " '不得了',\n",
       " '不得公开',\n",
       " '不得已',\n",
       " '不得而知',\n",
       " '不得要领',\n",
       " '不必',\n",
       " '不快',\n",
       " '不怀好意',\n",
       " '不怎么',\n",
       " '不怕',\n",
       " '不悦',\n",
       " '不惜',\n",
       " '不想',\n",
       " '不感兴趣',\n",
       " '不愧',\n",
       " '不愿',\n",
       " '不慌不忙',\n",
       " '不懊悔',\n",
       " '不成',\n",
       " '不成问题',\n",
       " '不打自招',\n",
       " '不拘小节',\n",
       " '不挺',\n",
       " '不提',\n",
       " '不撑',\n",
       " '不放',\n",
       " '不放过',\n",
       " '不敢',\n",
       " '不敢相信',\n",
       " '不料',\n",
       " '不断',\n",
       " '不断扩大',\n",
       " '不无',\n",
       " '不时',\n",
       " '不明',\n",
       " '不易',\n",
       " '不是',\n",
       " '不暗',\n",
       " '不曾',\n",
       " '不服气',\n",
       " '不来',\n",
       " '不查',\n",
       " '不止',\n",
       " '不正经',\n",
       " '不死心',\n",
       " '不求',\n",
       " '不求上进',\n",
       " '不测',\n",
       " '不济',\n",
       " '不清',\n",
       " '不清不楚',\n",
       " '不满',\n",
       " '不点',\n",
       " '不然',\n",
       " '不爽快',\n",
       " '不理',\n",
       " '不用',\n",
       " '不用说',\n",
       " '不由得',\n",
       " '不由自主',\n",
       " '不留',\n",
       " '不疼',\n",
       " '不痒',\n",
       " '不白住',\n",
       " '不相干',\n",
       " '不相往来',\n",
       " '不看',\n",
       " '不眠不休',\n",
       " '不着痕迹',\n",
       " '不知',\n",
       " '不知不觉',\n",
       " '不知去向',\n",
       " '不知就里',\n",
       " '不知情',\n",
       " '不知所措',\n",
       " '不祥',\n",
       " '不禁',\n",
       " '不符',\n",
       " '不答',\n",
       " '不算',\n",
       " '不算什么',\n",
       " '不管',\n",
       " '不管怎样',\n",
       " '不管是谁',\n",
       " '不约而同',\n",
       " '不经意',\n",
       " '不缺',\n",
       " '不置可否',\n",
       " '不耐',\n",
       " '不耐烦',\n",
       " '不肯',\n",
       " '不胜其烦',\n",
       " '不能',\n",
       " '不能不',\n",
       " '不腻',\n",
       " '不自在',\n",
       " '不至于',\n",
       " '不致',\n",
       " '不舍',\n",
       " '不良',\n",
       " '不良分子',\n",
       " '不行',\n",
       " '不要',\n",
       " '不要紧',\n",
       " '不见',\n",
       " '不见得',\n",
       " '不见踪影',\n",
       " '不规矩',\n",
       " '不觉',\n",
       " '不解',\n",
       " '不计成本',\n",
       " '不让',\n",
       " '不讲理',\n",
       " '不许',\n",
       " '不该',\n",
       " '不说',\n",
       " '不负众望',\n",
       " '不负责任',\n",
       " '不费吹灰之力',\n",
       " '不起',\n",
       " '不起眼',\n",
       " '不足',\n",
       " '不足为奇',\n",
       " '不足以',\n",
       " '不输',\n",
       " '不输给',\n",
       " '不过',\n",
       " '不远',\n",
       " '不远处',\n",
       " '不适',\n",
       " '不通',\n",
       " '不遗余力',\n",
       " '不配',\n",
       " '不酸',\n",
       " '不锁',\n",
       " '不锈钢',\n",
       " '不错',\n",
       " '不闻不问',\n",
       " '不难',\n",
       " '不难想象',\n",
       " '不需',\n",
       " '不顾',\n",
       " '不顾一切',\n",
       " '不高',\n",
       " '与',\n",
       " '与会人士',\n",
       " '与会者',\n",
       " '与其',\n",
       " '与其说是',\n",
       " '与桐',\n",
       " '与此同时',\n",
       " '丑八怪',\n",
       " '丑到',\n",
       " '丑态',\n",
       " '丑恶',\n",
       " '丑角',\n",
       " '丑陋',\n",
       " '专业',\n",
       " '专业书籍',\n",
       " '专业人士',\n",
       " '专业知识',\n",
       " '专业课',\n",
       " '专做',\n",
       " '专利',\n",
       " '专利权',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理文本，词典数字对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建从非重复字符到索引的映射\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text]) # 将文本转换为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\r\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '&' :   2,\n",
      "  \"'\" :   3,\n",
      "  '-' :   4,\n",
      "  '.' :   5,\n",
      "  '0' :   6,\n",
      "  '007':   7,\n",
      "  '0K':   8,\n",
      "  '0VER':   9,\n",
      "  '1' :  10,\n",
      "  '10':  11,\n",
      "  '11':  12,\n",
      "  '12':  13,\n",
      "  '13':  14,\n",
      "  '14':  15,\n",
      "  '14Mega':  16,\n",
      "  '169800':  17,\n",
      "  '2' :  18,\n",
      "  '2.5':  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', '第一章', '\\r\\n', ' ', ' ', ' ', ' ', '1', '\\r\\n', ' '] ---- characters mapped to int ---- > [    1     1     1     1 12331     0     1     1     1     1    10     0\n",
      "     1]\n"
     ]
    }
   ],
   "source": [
    "# 显示文本首 13 个字符的整数映射\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练样本和目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      "第一章\n"
     ]
    }
   ],
   "source": [
    "# 设定每个输入句子长度的最大值\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 创建训练样本 / 目标\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 构建数据集\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'    第一章\\r\\n    1\\r\\n    出了近铁布施站，沿着铁路径直向西。已经十月了，天气仍闷热难当，地面也很干燥。每当卡车疾驰而过，扬起的尘土极可能会让人又皱眉又揉眼睛。\\r\\n    笹垣润三的脚步说不上轻快。他今天本不必出勤。很久没休假了，还以为今天可以悠游地看点书。为了今天，他特地留着松本清张'\n",
      "'的新书没看。\\r\\n    公园出现在右边，大小足以容纳两场三垒棒球开打，丛林越野游戏、秋千、滑梯等常见的游乐设施一应俱全。这座公园是附近最大的一座，叫真澄公园。\\r\\n    公园后面有一栋兴建中的七层建筑，乍看之下平淡无奇，但笹垣知道里面几乎空无一物。在调到大阪警察本部之前，他就待在管辖这一带的西布施分局。\\r\\n    看热闹的人'\n",
      "'动作很快，已经聚集在大楼前，停在那里的好几辆警车几乎被看客团团围住。\\r\\n    笹垣没有直接走向大楼，而是在公园前右转。转角数来第五家店挂着“烤乌贼饼”的招牌，店面仅一叠大小。烤乌贼饼的台子面向马路，后面坐着一个五十岁左右的胖女人，正在看报。店内看来是卖零食的，但没见到小孩子的身影。\\r\\n    “老板娘，给我烤一片'\n",
      "'。”笹垣出声招呼。\\r\\n    中年妇人急忙合起报纸。“好，来了来了。”\\r\\n    妇人站起身，把报纸放在椅子上。笹垣衔了根和平牌香烟，擦火柴点着，瞄了一下那份报纸，看到“厚生省公布市场海鲜汞含量检查结果”的标题，旁边以小字写着“大量食用鱼类亦不致达到该含量”。\\r\\n    三月时，法院对'\n",
      "'熊本水俣病作出判决，与新泻水俣病、四日市哮喘病、痛痛病合称四大公害的审判，就此全数结案。结果，每一桩诉讼均是原告胜诉，这使得民众莫不对公害戒慎恐惧。尤其是日常食用的鱼类遭汞或PCB（多氯联苯）污染疑虑未消，使大众人心惶惶。\\r\\n    乌贼不会有问题吧？笹垣看着报纸想。\\r\\n    烤乌贼饼的两片铁板由铰链连在一起，夹住裹了面粉和蛋汁'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) \n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '    第一章\\r\\n    1\\r\\n    出了近铁布施站，沿着铁路径直向西。已经十月了，天气仍闷热难当，地面也很干燥。每当卡车疾驰而过，扬起的尘土极可能会让人又皱眉又揉眼睛。\\r\\n    笹垣润三的脚步说不上轻快。他今天本不必出勤。很久没休假了，还以为今天可以悠游地看点书。为了今天，他特地留着松本'\n",
      "Target data: '   第一章\\r\\n    1\\r\\n    出了近铁布施站，沿着铁路径直向西。已经十月了，天气仍闷热难当，地面也很干燥。每当卡车疾驰而过，扬起的尘土极可能会让人又皱眉又揉眼睛。\\r\\n    笹垣润三的脚步说不上轻快。他今天本不必出勤。很久没休假了，还以为今天可以悠游地看点书。为了今天，他特地留着松本清张'\n",
      "Step    0\n",
      "  input: 1 (' ')\n",
      "  expected output: 1 (' ')\n",
      "Step    1\n",
      "  input: 1 (' ')\n",
      "  expected output: 1 (' ')\n",
      "Step    2\n",
      "  input: 1 (' ')\n",
      "  expected output: 1 (' ')\n",
      "Step    3\n",
      "  input: 1 (' ')\n",
      "  expected output: 12331 ('第一章')\n",
      "Step    4\n",
      "  input: 12331 ('第一章')\n",
      "  expected output: 0 ('\\r\\n')\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批大小\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词集的长度\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 的单元数量\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x00000209AE2489D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x00000209AE2489D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x00000209AE2489D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x00000209AE2489D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x00000209AE270708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x00000209AE270708>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x00000209AE270708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x00000209AE270708>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Fail to find the dnn implementation. [Op:CudnnRNN]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f9a0ce44d1ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_example_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_example_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mexample_batch_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_example_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_batch_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"# (batch_size, sequence_length, vocab_size)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    246\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[1;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    751\u001b[0m                                 ' implement a `call` method.')\n\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[1;32m--> 343\u001b[1;33m           inputs, initial_state, training, mask)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[1;34m(self, inputs, initial_state, training, mask)\u001b[0m\n\u001b[0;32m    395\u001b[0m       \u001b[1;31m# Under eager context, check the device placement and prefer the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcudnn_gru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcudnn_gru_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandard_gru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnormal_gru_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcudnn_gru\u001b[1;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)\u001b[0m\n\u001b[0;32m    540\u001b[0m     outputs, h, _, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n\u001b[0;32m    541\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_c\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         rnn_mode='gru')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m   \u001b[0mlast_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn\u001b[1;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0minput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             ctx=_ctx)\n\u001b[0m\u001b[0;32m    139\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn_eager_fallback\u001b[1;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\u001b[0m\n\u001b[0;32m    224\u001b[0m   \"is_training\", is_training)\n\u001b[0;32m    225\u001b[0m   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\n\u001b[1;32m--> 226\u001b[1;33m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m    227\u001b[0m   _execute.record_gradient(\n\u001b[0;32m    228\u001b[0m       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Fail to find the dnn implementation. [Op:CudnnRNN]"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
