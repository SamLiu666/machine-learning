# 绪论

第1章至第9章主要介绍统计自然语言处理的理论，第10章至第15章主要介绍统计自然语言处理的应用。

- 在统计自然语言处理的理论方面，首先介绍有关的基础知识，例如，概率论和信息论的基本概念、形式语言和自动机的基本概念。
- 分别介绍了统计机器翻译、语音翻译、文本分类、信息检索与问答系统、信息抽取、口语信息处理与人机对话系统等各种应用系统中的统计自然语言处理方法。

本书讲述的是统计自然语言处理的经验主义方法，不要忘记在自然语言处理中还存在着另外一个方面，这就是基于规则的理性主义方法，我们也应当学习这些基于规则的理性主义方法，并且把这两种方法结合起来，彼此取长补短，使之相得益彰。

## 中文信息处理

- 汉字信息处理
- 汉语信息处理

困境： 歧义消解（disambiguation）

基本方法：

- 理性主义（rationalist）：建立符号处理系统
- 经验主义（empiricist）：通过语料库刻画真实的语言

# 概念、工具、数据

## 预备知识

最大似然估计（maximum likelihood estimation）：用相对频率作为概率的估计值

贝叶斯决策理论：似然比 $l(x) =\frac{p(x|w_1)}{p(x|w_2)} >\frac{P(w_2)}{p(w_1)}$ 则 $x\in{w_1}$， 否则 $x\in{w_2}$

熵：描述一个随机变量的不确定性的数量，越大约不确定熵最大的情况真实反映了时间的分布情况 $H(D) = - \sum_{k=1}^k \frac{|C_k|}{|D|} log_2 \frac{|C_k|}{|D|} = -\sum_{k=1}^{k} {p(x)}log_2 {p(x)} $  ， 则

$$ \hat{p} = argmax_{p\in{C}}H(p)$$

联合熵：一对随机变量平均所需的信息量

 $H(X,Y) = -\sum_{x\in{X}}\sum_{y\in{Y}}p(x,y)log{p(x,y)}$

熵连锁规则： $H(X_1,X_2,...,X_n)= H(X_1) + H(X_2|X_1)+..+H(X_n|X_1,..X_{n-1}) $

互信息：词汇聚类，汉语分词，$I(X;Y)$理解为Y的值透露了多少关于X的信息量

相对熵:（relative entropy）又称Kullback-Leibler差异（Kullback-Leibler divergence），或简称KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。当两个随机分布的差别增加时，其相对熵期望值也增大

交叉熵:交叉熵的概念就是用来衡量估计模型与真实概率分布之间差异情况的。 交叉熵越小，模型表现越好

困惑度：通常用困惑度（perplexity）来代替交叉熵衡量语言模型的好坏

噪声信道模型：如何定量地估算从信道输出中获取多少信息量。

### 支持向量机

SVM- Support Vector Machine ：短语识别，语义消歧，文本自动分类，信息过滤等

- 线性分类
  - 如果训练数据可以被无误差地划分，那么，以最大间隔分开数据的超平面称为最优超平面
  - $f(x) = <w {x}> + b= \sum_{i=1}^{n}w_ix_i + b$
  - $c(x) = argmax_{1<=i<=m}(<w_i x> + b_i)$  - 多分类问题
- 线性不可分
  - $f(x) = \sum_{i=1}^{i}a_iy_i<\Theta(x)\Theta(x)> + b$
  - $<\Theta(x)\Theta(x)>$ 为核函数

## 形式语言与自动机

# 关键技术

# 应用系统

