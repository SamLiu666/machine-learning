# 1 文字和语言 VS 数字和信息

# 2 自然语言处理-从规则到统计

句法分析-语义分析，自然语言与上下文相关

例子：乔丹喜欢打篮球

句子分为主语、谓语、句号三部分。然后进一步分析，得到语法分析树（Parse Tree)

# 3 统计语言模型

序列表示句子，根据序列出现的概率的大小判断这个句子出现的概率

Q：为什么N的取值比较小？

A：N元模型的大小几乎是N的指数函数，而且当N从1到2，从2到3模型的效果上升显著，而再增大N时，效果不是很显著了

Q：模型的训练、零概率问题和平滑方法

A：古德图灵平滑处理：对于未发生过的事情，概率不能为0，应该从概率的总量中，分配一个很小的比列给这些没有看见的事件

# 4 谈谈分词

- 查字典的方法：二义性的问题
- 统计语言模型：

现状：中文分词在工业界研究得差不多了

# 5 隐马尔科夫模型

通信模型：信息上下文 --》 编码  --》 传递的信息（信道） --》 解码 --》 接受的信息

根据接收端的观测信号推测信号源发送的信息：只需要找出所有原信息中最有可能产生出观测信息的那个信息，即已知

$o_1,o_2,o_3...$ 的情况下，求得令条件概率$P(s_1,s_2,s_3.. | o_1,o_2,o_3..)$达到最大值的信息：$s_1,s_2,s_3..$

$$s_1,s_2,s_3.. = ArgMax P(s_1,s_2,s_3.. | o_1,o_2,o_3..)$$ 模型转化为

$P( o_1,o_2,o_3.. |s_1,s_2,s_3..)\cdot p(S_1,S_2,S_3..)$ 可用马尔可夫模型求解

1. 马尔可夫假设：现在状态$s_t$ 只跟之前的状态$s_{t-1}$有关
2. 马尔可夫链（过程）：符合马尔可夫假设的随机过程

隐马尔可夫模型-HMM：

$P(s_1,s_2..,o_1,o_2,..) = \prod_t P(s_t|s_{t-1}) \cdot P(o_t|s_t)$

# 6 信息的度量和作用

信息熵：不确定性的多少，单位是 Bit，定义： $H(X)= -\sum_{x\in X} { P(x) logP(x)}$，不确定性越大，熵越大，搞清楚所需的信息就越大

$H(X|Y)= -\sum_{x\in X，y\in Y} { P(x,y) logP(x|y)}$， $H(X)\geq H(X|Y)$

互信息：$I(X|Y)= -\sum_{x\in X，y\in Y} { P(x,y) log \frac{P(x,y)}{P(x)P(y)}} = H(X)-H(X|Y)$

相对熵（交叉熵）：衡量两个取值为正数的函数的相似性

1. 完全相同的函数的相对熵为0
2. 相对熵越大差异越大，反之越小
3. 对于PDF函数，取值大于0，相对熵可以度量两个随机本部的差异性

# 7 - 应用：搜索系统

## 7 现代语言处理

故事

##  8 简单之美：搜索引擎

搜索：下载、索引、排序

布尔代数： 篮球 and 中国 not 美国

索引：分布式存储到不同服务器

## 9 图论和网络爬虫

图论，BFS，DFS

Q：构建一个网络爬虫？

A：DFS or BFS， 下载优先级排序的问题； 页面分析和URL提取；URL 表

## 10 PageRank

民主投票

## 11 确定网页相关性和查询相关性

TF-IDF

## 12 有限状态机和动态规划

## 14 余弦相似度 文本分类

## 15 奇异值分解

